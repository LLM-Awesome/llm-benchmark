# llm-benchmark


https://longbench2.github.io/#leaderboard
LongBench v2
Benchmarking Deeper Understanding and Reasoning on Realistic Long-context Multitasks
LongBench Team



https://livebench.ai/#/
LiveBench
A Challenging, Contamination-Free LLM Benchmark
LiveBench will appear as a Spotlight Paper in ICLR 2025.
This work is sponsored by Abacus.AI


https://llm-stats.com/

LLM Leaderboard
Analyze and compare AI models across benchmarks, pricing, and capabilities.


https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard
Open LLM Leaderboard Archived
Comparing Large Language Models in an open and reproducible way


https://artificialanalysis.ai/leaderboards/models

LLM Leaderboard - Comparison of GPT-4o, Llama 3, Mistral, Gemini and over 30 models
Comparison and ranking the performance of over 30 AI models (LLMs) across key metrics including quality, price, performance and speed (output speed - tokens per second & latency - TTFT), context window & others. For more details including relating to our methodology, see our FAQs.


https://www.vellum.ai/llm-leaderboard
LLM Leaderboard
This LLM leaderboard displays the latest public benchmark performance for SOTA model versions released after April 2024. The data comes from model providers as well as independently run evaluations by Vellum or the open-source community. We feature results from non-saturated benchmarks, excluding outdated benchmarks (e.g. MMLU). If you want to evaluate these models on your use-cases, try Vellum Evals.
